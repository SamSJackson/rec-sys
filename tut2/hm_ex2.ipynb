{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inwi1o3992TW"
      },
      "source": [
        "# Exerise 2\n",
        "> This exercise builds on the lectures' material, namely Lectures 3, 4, 6, 7 and 8.\n",
        "\n",
        "The aims of this execise are to:\n",
        " - Make your first recommender by implementing a user-based collaborating filtering (CF) (c.f. Lecture 4).\n",
        " - Make your first recommendations using Spotlight recommender toolkit on explicit data (c.f. Lecture 8).\n",
        " - Develop and evaluate baseline recommender systems (c.f. Lecture 3).\n",
        " - Start to think about explicit vs. implicit learners.\n",
        " - Evaluate your results using Spotlight (c.f. Lecture 6 & 7).\n",
        "\n",
        "\n",
        "There are 10 tasks to increase your understanding of the content of the Recommender Sytems course.  Each of these tasks have corresponding questions in the quiz.\n",
        "\n",
        "NB: Parts A, B & C are independent. It is important to properly manage your time to ensure that you have time to answer all parts of the Exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys992pU79yhD"
      },
      "source": [
        "#Standard setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import List, Tuple, Sequence\n",
        "SEED=20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Vlbi9W-d2j"
      },
      "source": [
        "We'll be using Movielens again. Let's load it in to the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg093kVRAvHw"
      },
      "source": [
        "!curl -o ml-latest-small.zip http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "# backup location\n",
        "#!curl -o ml-latest-small.zip http://www.dcs.gla.ac.uk/~craigm/recsysHM/ml-latest-small.zip\n",
        "!unzip -o ml-latest-small.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yss68U_EA0w2"
      },
      "source": [
        "ratings_df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "movies_df = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
        "\n",
        "# we're going to treat userId as strings, and similarly as movies. This will prevent confusion later on.\n",
        "ratings_df['userId'] = \"u\" + ratings_df['userId'].astype(str)\n",
        "ratings_df['movieId'] = \"m\" + ratings_df['movieId'].astype(str)\n",
        "movies_df['movieId'] = \"m\" +  movies_df['movieId'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5QQBZ3QS2Hv"
      },
      "source": [
        "ratings_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMfsveWjS4xP"
      },
      "source": [
        "movies_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rAIXwDoBCda"
      },
      "source": [
        "# Part A. User-based CF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isjp_rS0BCgh"
      },
      "source": [
        "You can generate a matrix of ratings with the ratings_df dataframe. Note that in the matrix, the unrated items are filled with 0 (this means they have no impact upon the calculated Cosine value, but you need to be careful about them in other situations)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKsmW549BNlq"
      },
      "source": [
        "r_df_matrix = ratings_df.pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n",
        "r_df_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpJXEqQ_BKxg"
      },
      "source": [
        "The left hand bold column is the [index of the dataframe](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) - that is, an attribute of the dataframe that allows fast lookup of rows. In this case, userId has become our index column.\n",
        "\n",
        "You can get all the index of users using the .index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRIXYwQgBRt2"
      },
      "source": [
        "r_df_matrix.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8RcZo1bBU8O"
      },
      "source": [
        "You can also use [.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) to access rows, by their \"index\". For instance, we can get all ratings of a specific user with userId=‘u1’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77XOSOdqBXmw"
      },
      "source": [
        "r_df_matrix.loc['u1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 0 - Matrix Analysis\n",
        "\n",
        "This task is concerned with examining `r_df_matrix`. Lets define *density* as the percentage of the matrix that that has been filled in (i.e. contains user interactions). How dense is `r_df_matrix`? Express your answer as a percentage (in range 0 to 100), rounded to 2 decimal places.\n",
        "\n",
        "Hints:\n",
        " - You can obtain a Numpy tensor using from a DataFrame `.to_numpy()` if you prefer\n",
        " - Similarly, you can ask for the `.shape` of a dataframe"
      ],
      "metadata": {
        "id": "cljwm-QyZy6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your solution here"
      ],
      "metadata": {
        "id": "d8XScXBN0C2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9_SlC4Beut"
      },
      "source": [
        "User-based CF heavily relies upon Cosine similarity. We are providing a Cosine similarity implementation based on numpy operations. We also show how to use `df.loc` to get all the ratings of a given user from `r_df_matrix` as a Series - we then make this into a numpy array using the [.values](https://pandas.pydata.org/docs/reference/api/pandas.Series.values.html) property.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NXXbCQsBbJW"
      },
      "source": [
        "def cos_sim(a, b):\n",
        "  from numpy.linalg import norm\n",
        "  from numpy import dot\n",
        "  return dot(a, b)/(norm(a)*norm(b))\n",
        "\n",
        "print('Cosine similarity between userId=1 and itself is:')\n",
        "print(cos_sim(r_df_matrix.loc['u1'].values, r_df_matrix.loc['u1'].values))\n",
        "\n",
        "print('Cosine similarity between userId=1 and userId=607 is:')\n",
        "print(cos_sim(r_df_matrix.loc['u1'].values, r_df_matrix.loc['u607'].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B061AiTiBlWJ"
      },
      "source": [
        "## Task 1. Get the most similar users.\n",
        "\n",
        "User-based CF is based on user-neighbourhoods. In this task, you will implement a function ` get_most_similar_users(userId : str, k : int = 10)` that will identify the userIds of the k most similar users to the specified userId, and their corresponding cosine similarities.\n",
        "\n",
        "In determining the most similar users, you should break ties based on their position in the array - for instance, if two users are tied as 2nd most similar user, the user who appears earlier should be 2nd, and the latter user third.\n",
        "\n",
        "You should exclude the compared user itself when generating a list of the most similar users.\n",
        "\n",
        "NB: We are using Python type hints to remind you what the function parameters (`str`, `int`) and return type (`Tuple[Sequence[str], Sequence[float]]`) should be.\n",
        "\n",
        "Hints:\n",
        " - The `cos_sim` function should be used here.\n",
        " - Higher `cos_sim` means more similar.\n",
        " - Try SciPy's [`rankdata()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html) function. Given an array, `rankdata()` tells you positions in sorted rank order. For instance:\n",
        "```\n",
        ">>> rankdata([5.9, 2.1, 4.3])\n",
        "array([3., 1., 2.])\n",
        "```\n",
        "It also has support for addressing ties.\n",
        " - The first return component of [np.nonzero](https://numpy.org/doc/stable/reference/generated/numpy.nonzero.html) can be used to return the indices of the elements that are non-zero. E.g.\n",
        " ```\n",
        " >>> np.array([True,False]).nonzero()[0]\n",
        "array([0])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj1wpnS7BbMF"
      },
      "source": [
        "from scipy.stats import rankdata\n",
        "\n",
        "def get_most_similar_users(userId : str, k : int = 10) -> Tuple[Sequence[str], Sequence[float]]:\n",
        "  # Add your solution here\n",
        "\n",
        "  topk_userids = [\"u?\"] # a list/numpy array of k userIds of top-k users\n",
        "  topk_cosines = [0.0]  # a list/numpy array of k cosine similarity values\n",
        "  return (topk_userids, topk_cosines)\n",
        "\n",
        "print(get_most_similar_users(userId='u3', k=1))\n",
        "\n",
        "# Add your solution here (cosine similarity > 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3jnW2BCAIWR"
      },
      "source": [
        "You can now answer the questions corresponding to Task 1 in the quiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9VnyzFLDtmN"
      },
      "source": [
        "print(get_most_similar_users(userId='u10', k=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVnsPTKjzqnD"
      },
      "source": [
        "print(get_most_similar_users(userId='u10', k=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIMwvzqJ2YuE"
      },
      "source": [
        "print(get_most_similar_users(userId='u500', k=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ZepSCnBwc7"
      },
      "source": [
        "## Task 2. Predict ratings via user-based CF.\n",
        "\n",
        "Now you should implement your user-based CF, within a `predict_rating()` function. The aim of this function is to predict the rating of a given userId for a given itemId.\n",
        "\n",
        "Your implementation should make use of your `get_most_similar_users()` implementation above, using k=10 nearest neighbours.\n",
        "- **If a neighbour has not rated a movie (e.g. rating = 0), you should skip that neighbour.**\n",
        "- **If you are unable to make a predicted rating, you should return 0.**\n",
        "\n",
        "Hint:\n",
        " - You may wish to revise user-based CF from Lecture 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eX33RkTBbS2"
      },
      "source": [
        "def predict_rating(userId : str, movieId : str) -> float:\n",
        "  # Add your solution here\n",
        "  predicted = \"?\" # predicted rating value\n",
        "  return predicted\n",
        "\n",
        "print(\"Predicted rating:\", predict_rating(userId='u1', movieId='m1'))\n",
        "\n",
        "print(\"Actual rating:\", r_df_matrix.loc['u1']['m1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuWLuAZzB9br"
      },
      "source": [
        "You can complete answering the quiz questions for Task 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqfUghVsCA7G"
      },
      "source": [
        "## Task 3. Predict ratings via user-based CF with Mean-center normalisation.\n",
        "\n",
        "Users usually rate differently: (1) some rate high, while others low. (2) Some use more of the scale than others. However, the user-based CF we implemented above ignores these differences. To this end, we can apply normalisation to compensate. In this task, you will implement user-based CF with Mean-Center Normalisation.\n",
        "\n",
        "Provide implementations for `mean_rating(userId : str)` and `predict_rating_MC(userId : str, movieId : str)`.\n",
        "\n",
        "Hints:\n",
        "- See lecture 4 about user-based CF with Mean-center normalisation.\n",
        "- Check if the predicted rating for a given user makes sense (i.e. what did the user rate before)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q5Efb4zB5_O"
      },
      "source": [
        "def mean_rating(userId : str) -> float:\n",
        "  # Add your solution here\n",
        "  mean_rating = '?' # mean-centering value\n",
        "  return mean_rating\n",
        "\n",
        "print(\"Mean rating of user u5:\", mean_rating('u5') )\n",
        "\n",
        "def predict_rating_MC(userId : str, movieId : str) -> float:\n",
        "  # Add your solution here\n",
        "  predicted = \"?\" # predicted rating value with mean-centering\n",
        "  return predicted\n",
        "\n",
        "print(\"Predicted rating:\", predict_rating_MC('u1', 'm1'))\n",
        "print(\"Actual rating:\", r_df_matrix.loc['u1']['m1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VApK70ld1tQ_"
      },
      "source": [
        "Now answer the questions for Task 3 in the quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_OpdQYHmTb"
      },
      "source": [
        "#Part B - Explicit Matrix Factorisation using Spotlight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17h6DqmGGh2f"
      },
      "source": [
        "In this part, we will investigate explicit matrix factorisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0oobMGk5eqv"
      },
      "source": [
        "We're going to use the Spotlight library - see https://github.com/maciejkula/spotlight - and its documentation at https://maciejkula.github.io/spotlight/\n",
        "\n",
        "You can install this direct from Git, but using Craig's patched version as done below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDObURPI-Shz"
      },
      "source": [
        "!pip install git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqoiteq7IJzJ"
      },
      "source": [
        "Now we can get onto some real recommendation work. Spotlight has a handy [Interactions](https://maciejkula.github.io/spotlight/interactions.html) object, which encapsulates the basics of a recommendation dataset.\n",
        "\n",
        "In fact, there are handy loaders for a few standard datasets including MovieLens, but let's make our own, so that we can match back to the dataframe.\n",
        "\n",
        "Interactions need *numbers* to uniquely identify each item and user. Unfortunately, our MovieLens uses numbers, but these aren't consecutive (i.e. we have missing movieIds values). They are also strings (i.e. movieIds start with \"m\" and userIds start with \"u\").\n",
        "\n",
        "Hence, for both movies and users, we have to assign numbers that start from 0. We will call these **iids** and **uids**.\n",
        "\n",
        "We use [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) to convert the MovieLens strings down to consecutive integers for use in Spotlight, in the `uid_map` and `iid_map` objects. We'll keep the reverse mapping around too, in case we want to lookup the actual movieId given the uid recorded by Spotlight (etc).\n",
        "\n",
        "*NB*: This is a *really* important concept to understand. Put simply, WE -- as humans -- deal with external representations (userId, movieId, in this dataset prefixed with \"u\" and \"m\" respectively). On the other hand, Spotlight can only deal with integers starting from 0 for both items and users (we call these \"iids\" and \"uids\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF89PzxNHrHq"
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "#create userId -> uid mapping dictionary. the next assigned value is the current size.\n",
        "uid_map = defaultdict(count().__next__)\n",
        "#ditto for movieId -> iid\n",
        "iid_map = defaultdict(count().__next__)\n",
        "\n",
        "#uids is an array of integers corresponding to the userId for every row in ratings_df\n",
        "#uid_map does the assignment of new uid values, or reusing the uid value assigned for\n",
        "#each userId\n",
        "uids = np.array([uid_map[uid] for uid in ratings_df[\"userId\"].values ], dtype=np.int32)\n",
        "#similar for iids\n",
        "iids = np.array([iid_map[iid] for iid in ratings_df[\"movieId\"].values ], dtype=np.int32)\n",
        "\n",
        "#freeze uid_map and iid_map so no more mapping are created\n",
        "uid_map.default_factory = None\n",
        "iid_map.default_factory = None\n",
        "\n",
        "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
        "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
        "num_items = len(iid_map)\n",
        "num_users = len(uid_map)\n",
        "\n",
        "print(\"%d users %d item\" % (num_users, num_items))\n",
        "\n",
        "ratings = ratings_df[\"rating\"].values.astype(np.float32)\n",
        "timestamps = ratings_df[\"timestamp\"].values.astype(np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be clear, `uid_map` and `iid_map` are just dictionaries - you can use them to lookup the uid (iid) assigned to a given user (movie).\n",
        "\n",
        "Similarly, `uid_rev_map` (`iid_rev_map`) can be used to recover the userId (movieId) for a given uid (iid)."
      ],
      "metadata": {
        "id": "q6pN0txDDPqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"userId %s got uid %d\" % (\"u556\", uid_map[\"u556\"]))\n",
        "print(\"movieId %s got iid %d\" % (\"m54001\", iid_map[\"m54001\"]))"
      ],
      "metadata": {
        "id": "34mX05BkDO9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrhni5wSX7QP"
      },
      "source": [
        "Furthemore, we will use user u556 as one of our illustrative users. You will remember from Exercise 1 that they rated a number of fantasy movies highly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDwZYy2xY3-D"
      },
      "source": [
        "## On towards Matrix Factorisation (MF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMejAIwPNVrU"
      },
      "source": [
        "Now let's build a Spotlight [Interactions](https://maciejkula.github.io/spotlight/interactions.html) object. This contains everything that Spotlight needs to train a model. We can split it up randomly into train and test subsets\n",
        "\n",
        "NB: we use a SEED (20) to make our results reproducible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg2tNWwPIBfu"
      },
      "source": [
        "from spotlight.interactions import Interactions\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "dataset = Interactions(user_ids=uids,\n",
        "                                  item_ids=iids,\n",
        "                                  ratings=ratings,\n",
        "                                  timestamps=timestamps)\n",
        "\n",
        "#lets initialise the seed, so that its repeatable and reproducible\n",
        "train_valid, test = random_train_test_split(dataset, random_state=np.random.RandomState(SEED))\n",
        "train, valid = random_train_test_split(train_valid, random_state=np.random.RandomState(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1shoeRmWKXxS"
      },
      "source": [
        "Let's see how big the two datasets are. What is the train/test split percentage size?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcjOWJ-qIEge",
        "cellView": "code"
      },
      "source": [
        "print(train)\n",
        "print(valid)\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBnevuHnT57k"
      },
      "source": [
        "Here, you can see that following the collaborative filtering task model (see Lecture 6), all users, and all items, are present in both training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTyhLswNulTX"
      },
      "source": [
        "Now, you can think of the Interaction objects are being the partitions of the rating matrix. But we don't store it as a single big matrix. Instead, we record three one-dimensional arrays:\n",
        "\n",
        "  * one for the ids of the users\n",
        "  * one for the ids of the items\n",
        "  * one for the actual rating values.\n",
        "\n",
        "Each of these arrays is the size of the number of ratings (64534 for the training set).\n",
        "\n",
        "In essence, Interactions is a sparse matrix - for each rating, we record its x and y position, as well as the rating itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74iQtBOoUZJ1"
      },
      "source": [
        "print(train.item_ids.shape)\n",
        "print(train.user_ids.shape)\n",
        "print(train.ratings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V9Hx3dhUjI1"
      },
      "source": [
        "For instance, let's look at the first rating:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz46dNMkUrCC"
      },
      "source": [
        "print(\"uid %d gave iid %d a rating of %d\" % (train.user_ids[0], train.item_ids[0],train.ratings[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4R1mQgSUZ86"
      },
      "source": [
        "Let's take our favourite fantasy adventure fan from Exercise 1, userId u556. We can give a look at their training ratings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47QmgWtYvM4u"
      },
      "source": [
        "# map userId to the internal uid value\n",
        "userId = \"u556\"\n",
        "uid = uid_map.get(userId)\n",
        "\n",
        "# see which ratings are for this user. Use this to filter the item and ratings arrays.\n",
        "# here we are filtering a numpy array based on an array of True/False values. Its just\n",
        "# like filtering a Pandas data frame.\n",
        "print(train.item_ids[train.user_ids == uid])\n",
        "print(train.ratings[train.user_ids == uid])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhnKAa-KKclT"
      },
      "source": [
        "We can now learn a model. Let's start with a matrix factorisation for explicit data.  We train the model using the `fit` method. This is just like the `fit` in Sklearn - we're fitting  a model to the specified training data.\n",
        "\n",
        "This might take upto a minute.\n",
        "\n",
        "**NB:**  Spotlight can support using GPUs which we could use to slightly speed up training time, but that will make our life more difficult later on, so let's ignore this for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UduCmnlbKt-O"
      },
      "source": [
        "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
        "import time\n",
        "\n",
        "emodel = ExplicitFactorizationModel(n_iter=10,\n",
        "                                    embedding_dim=32, #this is Spotlight default\n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
        ")\n",
        "current = time.time()\n",
        "\n",
        "emodel.fit(train, verbose=True)\n",
        "\n",
        "end = time.time()\n",
        "diff = end - current\n",
        "print(\"Training took %d seconds \"% (diff))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTTK1BlbLL_t"
      },
      "source": [
        "How well did we do. Well, let's give a look at the recommentations, for our specific user, userId u556.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjyVPjoGLoy6"
      },
      "source": [
        "userId = \"u556\"\n",
        "\n",
        "# convert the string to the internal integer\n",
        "uid = uid_map.get(userId)\n",
        "print(\"One test item_id for userId %s (uid %d) is \" % (userId, uid))\n",
        "\n",
        "# pick one rating that the user made\n",
        "testItemId = test.item_ids[test.user_ids == uid][0]\n",
        "print(\"Test movieId is %s iid %d \" % (iid_rev_map.get(testItemId), testItemId ) )\n",
        "\n",
        "\n",
        "#here 0 is a dummy item, which Spotlight needs for some reason...\n",
        "#we discard its prediction using [1]\n",
        "predicted = emodel.predict( np.array([uid]), item_ids=np.array([0, testItemId]) )[1]\n",
        "\n",
        "#what was the actual score of the user for that movie?\n",
        "#we can get the appropriate row from the ratings dataframe, then extract that value\n",
        "actual = ratings_df[(ratings_df.movieId==iid_rev_map.get(testItemId)) & (ratings_df.userId==userId)][\"rating\"].values[0]\n",
        "\n",
        "\n",
        "def getMovieTitle(iid):\n",
        "  return movies_df[movies_df['movieId'] == iid_rev_map.get(iid)][\"title\"].values[0]\n",
        "\n",
        "print(\"Predicted rating for '%s' was %f, actual rating %0.1f, error was %f\" % (getMovieTitle(testItemId), predicted, actual, abs(predicted-actual) ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWdWW8QhacOw"
      },
      "source": [
        "So this is interesting - while we saw above that the users liked fantasy movies, we predicted a rating of $\\sim 2.5$, but the user gave this particular movie a 3.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgX01Wwlr_WA"
      },
      "source": [
        "We can also ask for **all** of the recommendations for a given user:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz28wrmIsDa-"
      },
      "source": [
        "allpreds = emodel.predict( np.array([uid]) )\n",
        "\n",
        "print(allpreds)\n",
        "print(allpreds.size)\n",
        "\n",
        "#we can recover the original rating for our test item\n",
        "print(allpreds[testItemId])\n",
        "\n",
        "# lets just check we got the correct prediction\n",
        "print(allpreds[testItemId] - actual < 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d25P7AtBPgXS"
      },
      "source": [
        "## Latent Factors aka Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyL5EG65TuNo"
      },
      "source": [
        "Let's see how these recommendations are made. Remember from Lecture 8 that the prediction is made based on the dot product of the user's and item's latent factors (also know as \"embeddings\").\n",
        "\n",
        "We can access these embeddings directly from the emodel object. Each embedding has 32 dimensions, which is what we set when configuring Spotlight's Explicit Factorisation Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf2Em9KSa74G",
        "cellView": "both"
      },
      "source": [
        "#the embedding of an item is a PyTorch tensor of size 32\n",
        "#a PyTorch tensor can be thought of having similar semantics as an numpy array.\n",
        "print(emodel._net.item_embeddings.weight[0].shape)\n",
        "emodel._net.item_embeddings.weight[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKlJIBoVNYr"
      },
      "source": [
        "We can check how Spotlight makes its prediction. The key line is https://github.com/maciejkula/spotlight/blob/master/spotlight/factorization/representations.py#L89\n",
        "\n",
        "This takes the (dot-)product of the user's \"embedding\" (latent factor) and the item's embedding. On top of these are added \"user_biases\" and \"item_biases\". What do you think these last two components are for?\n",
        "\n",
        "Let's reproduce this for our favourite user..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g14v62m4YRyd"
      },
      "source": [
        "# uid=555 for u556\n",
        "# testItemId is our item of interest\n",
        "\n",
        "dotprod = (emodel._net.user_embeddings.weight[uid] * emodel._net.item_embeddings.weight[testItemId]).sum(0)\n",
        "user_bias = emodel._net.user_biases(torch.tensor([uid]))\n",
        "item_bias = emodel._net.item_biases(torch.tensor([testItemId], dtype=torch.long))\n",
        "\n",
        "print(getMovieTitle(testItemId))\n",
        "\n",
        "dotprod + user_bias + item_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VE2tpcJb8e7"
      },
      "source": [
        "## Task 4. Examining Latent Factors\n",
        "\n",
        "Let's give a look at item-item similarities. Write a function `mostsimilar(targetMovieId, model)` that identifies the most similar movieId to the specified target, based on the Cosine similarity of their item embedding vectors.\n",
        "\n",
        "What's the closest movie to \"Harry Potter and the Deathly Hallows: Part 1 (2010)\" , which is movieId m81834 in the MovieLens dataset?\n",
        "\n",
        "Hint:\n",
        " - Since we're working with PyTorch tensors (rather than the numpy vectors used in Part A), you should use [`nn.functional.cosine_similarity(x, y, dim=0)`](https://pytorch.org/docs/stable/nn.functional.html#cosine-similarity) to calculate the cosine similarity between two vectors x & y, as demonstrated below between two orthogonal vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7fDGUx6fBR3"
      },
      "source": [
        "import torch.nn as nn\n",
        "nn.functional.cosine_similarity(\n",
        "     torch.tensor([1.0,0]),\n",
        "     torch.tensor([0,1.0],), dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rg_DuBnoMEu"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def mostsimilar(targetIId : int, model):\n",
        "  highest=0\n",
        "  highestCos=0\n",
        "\n",
        "  #you may assume that model._num_items provides the total number of items\n",
        "\n",
        "\n",
        "  # Add your solution here\n",
        "  #####################\n",
        "\n",
        "  print(train.num_items)\n",
        "  print(\"targetMovieId = %s '%s' (iid %d)\" % (iid_rev_map.get(targetIId), getMovieTitle(targetIId), targetIId))\n",
        "  print(\"mostSimilar = %s (iid %d) with cosine of %f \" % ( iid_rev_map.get(highest), highest, highestCos))\n",
        "\n",
        "\n",
        "mostsimilar(iid_map[\"m81834\"], emodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Br4nChVqUAj"
      },
      "source": [
        "Hopefully, you can see a correspondence between the nearest movie to `\"m81834\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgUvFR0AK8XI"
      },
      "source": [
        "mostsimilar(iid_map[\"m88125\"], emodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENhT5FXX3qH9"
      },
      "source": [
        "mostsimilar(iid_map[\"m44\"], emodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxHfMZgbcdRu"
      },
      "source": [
        "## Evaluating performance\n",
        "\n",
        "Finally, let's see how good we are at our rating predictions. Handily, Spotlight implements a few common evaluation measures for us to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB8UJykycm3G"
      },
      "source": [
        "from spotlight.evaluation import rmse_score\n",
        "\n",
        "train_rmse = rmse_score(emodel, train)\n",
        "test_rmse = rmse_score(emodel, test)\n",
        "\n",
        "print('Train RMSE {:.3f}, test RMSE {:.3f}'.format(train_rmse, test_rmse))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcRrTWx9lkoo"
      },
      "source": [
        "## Task 5. Tuning\n",
        "\n",
        "Now we wish to tune the latent factors. The task here is to train and evaluate new instances of ExplicitFactorizationModels using different numbers of latent factors, while leaving the other parameters unchanged (i.e. `n_iter=10, use_cuda=False, random_state=np.random.RandomState(SEED)`.\n",
        "\n",
        "You should also record the training times for different numbers of latent factors.\n",
        "\n",
        "You should vary the factors in `[8,16,32,64]`. Evaluate and record the RMSE values of the resulting models on (i) the training set (`train`), (ii) the  validation set (`valid`) and (iii) the test set (`test`). Use matplotlib to create a graph showing how the training, validation and test RMSE change as the number of latent factors is varied. Use [plt.savefig()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) to save a PNG of your graph.\n",
        "\n",
        "You can now answer the questions about Task 5 in the quiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmfqVqvcjgLi"
      },
      "source": [
        "# Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYTtH0_pl6sj"
      },
      "source": [
        "## Evaluating Other Models\n",
        "\n",
        "When evaluating models, it's important to compare to some reasonable baselines.\n",
        "\n",
        "Fortunately, Spotlight's `rmse_score()` method can be used to evaluate any Python object that adheres to the specification of the `predict()` function. For instance, we can make a baseline \"static\" scoring model, which returns the same scores for each user. This set of scores is passed as numpy array in the constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Xd1X4TX2Tq"
      },
      "source": [
        "class StaticModel:\n",
        "\n",
        "  def __init__(self, staticscores):\n",
        "    self.numitems = len(staticscores)\n",
        "    assert isinstance(staticscores, np.ndarray), \"Expected a numpy array\"\n",
        "    assert staticscores.dtype == np.float32 or staticscores.dtype == np.float64, \"Expected a numpy array of floats\"\n",
        "    self.staticscores = staticscores\n",
        "\n",
        "  # uids are the user(s) we are requesting recommendations for;\n",
        "  # returns an array of scores, one for each item\n",
        "  # the array is duplicated for each user requested\n",
        "  def predict(self, uids, iids=None):\n",
        "    # this model returns all zeros, regardless of userid\n",
        "\n",
        "    # we respond to one or more uids\n",
        "    uids = [uids] if isinstance(uids, int) else uids\n",
        "\n",
        "    # if iids is specificed, we filter predicts for those userids\n",
        "    # if iids is not specificed, predict() returns the prediction for each item _in iid order_\n",
        "    iids = np.arange(self.numitems) if iids is None else iids\n",
        "    return [self.staticscores[iids] for u in uids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhl_7mUxmdlv"
      },
      "source": [
        "For instance, we can make a static baseline that just returns 0 for every item, regardless of the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3FVJWJzYhoC"
      },
      "source": [
        "mydummymodel = StaticModel(np.zeros(num_items))\n",
        "\n",
        "print(\"Asking for 2 users, one item: \" + str(mydummymodel.predict([0,1],0)))\n",
        "print(\"Asking for one item: \" + str(mydummymodel.predict(0,0)))\n",
        "print(\"Asking for two items: \" + str(mydummymodel.predict(0,[0,1])))\n",
        "print(\"RMSE of our dummy model: %f\" % rmse_score(mydummymodel, test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSnTtOzAoA2l"
      },
      "source": [
        "## Task 6. Popularity-based Recommenders\n",
        "\n",
        "This task asks you to implement other baseline recommenders.\n",
        "\n",
        "**Using ratings_df**, create three new instances of StaticModel as baselines:\n",
        "\n",
        "(a). the number of ratings for each item - you must linearly normalise this to be in the range 0-5.\n",
        "\n",
        "(b). the number of 5 scores received by an item - you must linearly normalise this to be in the range 0-5.\n",
        "\n",
        "(c). the average rating value for each item (no need to normalise - scores are already 0-5)\n",
        "\n",
        "Evaluate your baseline models in terms of RMSE, as well as providing their scores for particular iids, as requested in the quiz.\n",
        "\n",
        "Hints:\n",
        " - You may find iterating over a dataframe using iterrows() useful - e.g. see  https://stackoverflow.com/a/16476974\n",
        " - The order that predict() returns scores for items is VERY IMPORTANT. Think carefully about the assumed order that predict() returns item scores for, and how you can recover that order when working with ratings_df."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEHtLIXLjHqx"
      },
      "source": [
        "# Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aCiYTWaeobQ"
      },
      "source": [
        "# Part C - Implicit Recommendation\n",
        "\n",
        "This part of the lab uses a music dataset from [Last.fm](https://www.last.fm/) -- a Spotify-like music streaming service -- that was obtained by a researcher at Pompeu Fabra University (Barcelona, Spain). The relevant citation is:\n",
        "\n",
        "```\n",
        "  @book{Celma:Springer2010,\n",
        "      \tauthor = {Celma, O.},\n",
        "      \ttitle = {{Music Recommendation and Discovery in the Long Tail}},\n",
        "       \tpublisher = {Springer},\n",
        "       \tyear = {2010}\n",
        "      }\n",
        " ```\n",
        "\n",
        "You can have more information about the dataset at [this link](http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsDavoa3qC64"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "The full Last.fm dataset is 2.4GB uncompressed. So we focus on a sample with 200k listens. You can download and load the sample into a DataFrame using just one line code:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listens_df = pd.read_csv(\"https://www.dcs.gla.ac.uk/~craigm/recsysH/lastfm-200ksample-listens_df.csv.gz\")"
      ],
      "metadata": {
        "id": "9FCrsO425d4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZSwFnZSgrNU"
      },
      "source": [
        "\n",
        "Let's look at the dataset. Note that the we don't have any explicit ratings by the users. We just know what they interacted with (and when)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAP3dPt-4KMi"
      },
      "source": [
        "listens_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLGzG9Rig3E6"
      },
      "source": [
        "## An implicit recommendation approach\n",
        "\n",
        "Let's move away from explicit recommendation to implicit.\n",
        "\n",
        "We will continue using the [Spotlight](https://github.com/maciejkula/spotlight/) toolkit for our recommender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U03zZ6CH---1"
      },
      "source": [
        "We can construct [Interaction](https://maciejkula.github.io/spotlight/interactions.html) objects for Spotlight in the same way as before. The only difference is that this time we do not record the user's ratings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcRhNWXzg7LT"
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "#we cant trust the musicbrainz ids to exist, so lets build items ids based on artist & trackname attributes\n",
        "LFMiid_map = defaultdict(count().__next__)\n",
        "LFMiids = np.array([LFMiid_map[str(artist)+\"/\"+str(trackname)] for artist,trackname in listens_df[[\"artist\",\"trackname\"]].values ], dtype=np.int32)\n",
        "\n",
        "LFMuid_map = defaultdict(count().__next__)\n",
        "LFMuids = np.array([LFMuid_map[uid] for uid in listens_df[\"user\"].values ], dtype=np.int32)\n",
        "#freeze uid_map and iid_map so no more mapping are created\n",
        "LFMuid_map.default_factory = None\n",
        "LFMiid_map.default_factory = None\n",
        "\n",
        "LFMuid_rev_map = {v: k for k, v in LFMuid_map.items()}\n",
        "LFMiid_rev_map = {v: k for k, v in LFMiid_map.items()}\n",
        "\n",
        "from spotlight.interactions import Interactions\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "#NB: we will set num_users and num_items here - its a good practice.\n",
        "imp_dataset = Interactions(user_ids=LFMuids, item_ids=LFMiids, num_users=len(LFMuid_map), num_items=len(LFMiid_map))\n",
        "#we could add the timestamps here if we were doing sequence recommendation\n",
        "\n",
        "#what have we got.\n",
        "print(imp_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22dmr7JKqUnz"
      },
      "source": [
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "itrain, itest = random_train_test_split(imp_dataset, random_state=np.random.RandomState(SEED))\n",
        "print(itrain)\n",
        "print(itest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFQazPPxhG2_"
      },
      "source": [
        "Let's run Spotlight's impllicit Matrix Factorisation on this dataset. Here, we use a *pointwise* loss, which just tries to predict whether the user will like the item or not. It does not use the BPR loss function (more on that later).\n",
        "\n",
        "**Warning**: this dataset is difficult for the learner - this *will* take a few minutes to learn... Use the time to read-on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co3ZwYgkhKvq"
      },
      "source": [
        "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
        "import time\n",
        "\n",
        "imodel = ImplicitFactorizationModel(n_iter=5,\n",
        "                                    embedding_dim=32, #this is Spotlight default\n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
        ")\n",
        "current = time.time()\n",
        "\n",
        "imodel.fit(itrain, verbose=True)\n",
        "end = time.time()\n",
        "diff = end - current\n",
        "print(\"Training took %d seconds\" % (diff))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zso4C5wehLog"
      },
      "source": [
        "Again, we can look at the predictions. We make a prediction (a score ) for ALL items for user uid 0. Note that the scores vary in magnitude - indeed, we're not predicting a rating, we just need to have scores in order to rank the items in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR_qbWXEhUDB"
      },
      "source": [
        "print(imodel.predict(0))\n",
        "print(len(imodel.predict(0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyZ3PyAxhdDF"
      },
      "source": [
        "Now that we have the scores of all items for a given user, we need to identify the top-scored ones, i.e. those that we would present to the user.\n",
        "\n",
        "## Task 7. Track Analysis\n",
        "\n",
        "Write a function `tracksForUser(user)` to identify the artist name & track of the top K (e.g. K=4) items based on their prediction scores of `imodel` for a given user index index (i.e. 0.. 964). What are the top scored 10 tracks recommended for user uid 4?\n",
        "\n",
        "Hints:\n",
        "\n",
        "\n",
        " - I also found [`np.argwhere()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argwhere.html) to be useful. It results only the positions of an array that are True. For instance:\n",
        "```\n",
        ">>> np.argwhere([True, False])\n",
        "array([[0]])\n",
        "```\n",
        " Alternatively, you can sort and then slice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBGdB7SOhcjp"
      },
      "source": [
        "#Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWFSAuQ40p2Y"
      },
      "source": [
        "## Task 8. Artist Analysis\n",
        "\n",
        "Look at the artists actually listened to by uid 4, and compare/contrast with the predictions of the recommender. It's useful to examine how many times each artist was listened to.\n",
        "\n",
        "Hints:\n",
        " - Use a groupby on a suitable subset of the `listens_df` dataframe.\n",
        " - Sort by descending frequency of listen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnNErTHP0u8G"
      },
      "source": [
        "#Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FWymqQRxKj"
      },
      "source": [
        "I observed that uid 4 listened frequently to \"Radiohead\" (rank 3), while a Radiohead song was among the top 10 ranked songs in our predicted model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmTNae6Romuk"
      },
      "source": [
        "## Evaluating an implicit recommender\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Q2TTpZuHON"
      },
      "source": [
        "We can examine the MRR of the implicit model we have learned. We pass it the test set (which contains knowledge of what the user *actually* clicked), as our ground truth.\n",
        "\n",
        "In the second variant, we also pass the training data. Give a look at the  implementation of [mrr_score()](https://github.com/cmacdonald/spotlight/blob/master/spotlight/evaluation.py#L8) to understand what it is doing, and why.\n",
        "\n",
        "**Questions for you to consider**\n",
        " - Why is the second score lower?\n",
        " - Would this be the same for all recommendation settings?\n",
        " - In the implementation, why are the scores negated, why do we use [rankdata()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html)?\n",
        "\n",
        "We will use the first variant for this Lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLV71LSjt-k2"
      },
      "source": [
        "from spotlight.evaluation import mrr_score\n",
        "\n",
        "#evaluate on this dataset takes approx 1 minute\n",
        "!date\n",
        "print(mrr_score(imodel, itest).mean())\n",
        "!date\n",
        "print(mrr_score(imodel, itest,  train=itrain).mean())\n",
        "!date\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjM_kZNtAE2k"
      },
      "source": [
        "How to interpret an MRR score - we know it has a range [0,1] with 1 being best. 1 means, on average across all users, we make a relevant prediction at rank 1; 0.5 means, on average, at rank 2. This is a very rough rule-of-thumb - MRR isn't a linear measure, so  a few poor predictions affect the average more than a few good ones.\n",
        "\n",
        "**More information:**\n",
        "\n",
        "rankdata() is a very useful function. Here's an example of its output:\n",
        "```python\n",
        ">>> rankdata([0, 2, 3, 2])\n",
        "array([1. , 2.5, 4. , 2.5])\n",
        "```\n",
        "\n",
        "It tells us the RANK of the number at each position. So the first element of the array (value 0) was the smallest, so is \"rank 1\"; the highest value gets \"rank 4\"; the other two values are tied, so they get equal ranks (2.5 is halfway between 2 & 3). We can adjust this tie-breaking behaviour using the `method=` kwarg.\n",
        "\n",
        "You can now answer all questions for Task 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL2jRiGF2uLb"
      },
      "source": [
        "## Task 9. Listens and Recommendations\n",
        "\n",
        "We now want to analyse how the recommender predictions differs from what it was trained on. This helps us understand in what situations does the recommender perform well or not.\n",
        "\n",
        "We can see the models performance by using `mrr_score(imodel, itest)`.\n",
        "\n",
        "*   Pick the user with the lowest uid that has the highest RR. How many listens (ie. how many times they have listened to any song) did they have in the training dataset (as represented by `itrain`)?\n",
        "*   Similarly, pick the user with the lowest uid that had the lowest RR. How many listens did they have in the training dataset (`itrain`)?\n",
        "\n",
        "Hints:\n",
        " * What does an Interaction object contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6H0l1S6nNcd"
      },
      "source": [
        "#Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anEhZSz9nPZt"
      },
      "source": [
        "Next, make a numpy array containing the number of listens for each uid in `itrain`. Plot a histogram of the distribution - like in Exercise 1, use matplotlib's histogram functionality, the default number of bins and use `log=True`.\n",
        "\n",
        "Save the PNG for uploading to the quiz when prompted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCI34-U48HHI"
      },
      "source": [
        "#Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kjpISm3nwAy"
      },
      "source": [
        "Many users have very few listens. Lets set 20 listens as a threshold.\n",
        "\n",
        "Lets define users with < 20 listens as cold-start users.\n",
        "Based on `itrain`, how many cold-start users are there?\n",
        "Looking back to our evaluation results, what is the MRR for ONLY these users, versus \"normal\" with 20 or more listens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VRv-AAReGDs"
      },
      "source": [
        "#Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR_l43rFc7eo"
      },
      "source": [
        "## Task 10 - BPR\n",
        "\n",
        "Finally, let's compare the *pointwise* implicit factorisation model with Bayesian Personalised Ranking (BPR). BPR is a very key recommendation model in the literature, which is widely used today as a baseline in many research papers.\n",
        "\n",
        "Train an ImplicitFactorizationModel on the Last FM dataset (i.e. `itrain`) using identical settings as before, except adding `loss='bpr'`. Record the time taken to train, and the evaluate its effectiveness in terms of MRR. Do NOT use the `train=itrain` argument to `mrr_score()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNeyn7o2tf54"
      },
      "source": [
        "#Add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTPM_BGcUc0"
      },
      "source": [
        "# End of Exercise\n",
        "\n",
        "As part of your submission, you should complete the Exercise 2 quiz on Moodle.\n",
        "You will need to upload your notebook, complete with the **results** of executing the code (inc figures and plots)."
      ]
    }
  ]
}
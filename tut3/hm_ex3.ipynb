{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THQFNe3zdt1f"
   },
   "source": [
    "# RecSys M/H 2023 - Exercise 3 Template\n",
    "\n",
    "The aims of this exercise are:\n",
    " - Explore a different recommendation dataset\n",
    " - Develop and evaluate baseline recommender systems\n",
    " - Implement hybrid recommender models\n",
    " - Explore diversification issues in recommender systems\n",
    " - Revise other material from the lectures.\n",
    "\n",
    "As usual, there is a corresponding Quiz on Moodle for this Exercise, which should be answered as you proceed. For more details, see the Exercise 3 specification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvTdMyXdODex"
   },
   "source": [
    "# Part-Pre. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww--_kl9-ndn"
   },
   "source": [
    "## Pre 1. Setup Block\n",
    "\n",
    "This exercise will use the [Goodreads]() dataset for books. These blocks setup the data files, Python etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFgYpbhh0tkX"
   },
   "outputs": [],
   "source": [
    "!rm -rf ratings* books* to_read* test*\n",
    "\n",
    "!curl -o ratings.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-ratings.csv\"\n",
    "!curl -o books.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-books.csv\"\n",
    "!curl -o to_read.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-to_read.csv\"\n",
    "!curl -o test.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VpVnNrZ1EiX"
   },
   "outputs": [],
   "source": [
    "#Standard setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "!pip install git+https://github.com/cmacdonald/spotlight.git@seed#egg=spotlight\n",
    "from spotlight.interactions import Interactions\n",
    "SEED=20\n",
    "BPRMF=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtJO0e0m-hun"
   },
   "source": [
    "## Pre 2. Data Preparation\n",
    "\n",
    "Let's load the `goodbooks` dataset into dataframes.\n",
    "- `ratings.csv`: It contains ratings sorted by time. Ratings go from one to five.\n",
    "- `to_read.csv`: It provides IDs of the books marked \"to read\" by each user, as <user_id, book_id> pairs.\n",
    "- `books.csv`: It has metadata for each book (goodreads IDs, authors, title, average rating, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKAb25iw1MYw"
   },
   "outputs": [],
   "source": [
    "#load in the csv files\n",
    "ratings_df = pd.read_csv(\"ratings.csv\")\n",
    "books_df = pd.read_csv(\"books.csv\")\n",
    "to_read_df = pd.read_csv(\"to_read.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-mJBqjjEajC"
   },
   "outputs": [],
   "source": [
    "## Test\n",
    "to_read_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6rqfn53OhDC"
   },
   "outputs": [],
   "source": [
    "#cut down the number of items and users\n",
    "counts=ratings_df[ratings_df[\"book_id\"] < 2000].groupby([\"book_id\"]).count().reset_index()\n",
    "valid_books=counts[counts[\"user_id\"] >= 10][[\"book_id\"]]\n",
    "\n",
    "books_df = books_df.merge(valid_books, on=\"book_id\")\n",
    "ratings_df = ratings_df[ratings_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "to_read_df = to_read_df[to_read_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "test = test[test[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "\n",
    "\n",
    "#stringify the id columns\n",
    "def str_col(df):\n",
    "  if \"user_id\" in df.columns:\n",
    "    df[\"user_id\"] = \"u\" + df.user_id.astype(str)\n",
    "  if \"book_id\" in df.columns:\n",
    "    df[\"book_id\"] = \"b\" + df.book_id.astype(str)\n",
    "\n",
    "str_col(books_df)\n",
    "str_col(ratings_df)\n",
    "str_col(to_read_df)\n",
    "str_col(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7cgXhmYUXIn"
   },
   "source": [
    "Here we construct the Interactions objects from `ratings.csv`, `to_read.csv` and `test.csv`. We manually specify the num_users and num_items parameters to all Interactions objects, in case the test set differs from your training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15ClgJOdTTt1"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "\n",
    "iid_map = defaultdict(count().__next__)\n",
    "\n",
    "\n",
    "rating_iids = np.array([iid_map[iid] for iid in ratings_df[\"book_id\"].values], dtype = np.int32)\n",
    "test_iids = np.array([iid_map[iid] for iid in test[\"book_id\"].values], dtype = np.int32)\n",
    "toread_iids = np.array([iid_map[iid] for iid in to_read_df[\"book_id\"].values], dtype = np.int32)\n",
    "\n",
    "\n",
    "uid_map = defaultdict(count().__next__)\n",
    "test_uids = np.array([uid_map[uid] for uid in test[\"user_id\"].values], dtype = np.int32)\n",
    "rating_uids = np.array([uid_map[uid] for uid in ratings_df[\"user_id\"].values], dtype = np.int32)\n",
    "toread_uids = np.array([uid_map[iid] for iid in to_read_df[\"user_id\"].values], dtype = np.int32)\n",
    "\n",
    "\n",
    "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
    "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
    "\n",
    "\n",
    "rating_dataset = Interactions(user_ids=rating_uids,\n",
    "                               item_ids=rating_iids,\n",
    "                               ratings=ratings_df[\"rating\"].values,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "toread_dataset = Interactions(user_ids=toread_uids,\n",
    "                               item_ids=toread_iids,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "test_dataset = Interactions(user_ids=test_uids,\n",
    "                               item_ids=test_iids,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "print(rating_dataset)\n",
    "print(toread_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "#here we define the validation set\n",
    "toread_dataset_train, validation = random_train_test_split(toread_dataset, random_state=np.random.RandomState(SEED))\n",
    "\n",
    "num_items = test_dataset.num_items\n",
    "num_users = test_dataset.num_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2noK30pBEsF"
   },
   "source": [
    "Finally, this is some utility code that we will use in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kDxZgICBFp6"
   },
   "outputs": [],
   "source": [
    "def getAuthorTitle(iid):\n",
    "  bookid = iid_rev_map[iid]\n",
    "  row = books_df[books_df.book_id == bookid]\n",
    "  return row.iloc[0][\"authors\"] + \" / \" + row.iloc[0][\"title\"]\n",
    "\n",
    "print(\"iid 0: \" + getAuthorTitle(0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt4I2C5DTUL5"
   },
   "source": [
    "## Pre 3. Example Code\n",
    "\n",
    "To evaluate some of your hand-implemented recommender systems (e.g. Q1, Q4), you will need to instantiate objects that match the specification of a Spotlight model, which `mrr_score()` etc. expects.\n",
    "\n",
    "\n",
    "Here is an example recommender object that returns 0 for each item, regardless of user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2eaxy_hakbC"
   },
   "outputs": [],
   "source": [
    "from spotlight.evaluation import mrr_score, precision_recall_score\n",
    "\n",
    "class dummymodel:\n",
    "\n",
    "  def __init__(self, numitems):\n",
    "    self.predictions=np.zeros(numitems)\n",
    "\n",
    "  #uid is the user we are requesting recommendations for;\n",
    "  #returns an array of scores, one for each item\n",
    "  def predict(self, uid):\n",
    "    #this model returns all zeros, regardless of userid\n",
    "    return( self.predictions )\n",
    "\n",
    "#lets evaluate how the effeciveness of dummymodel\n",
    "\n",
    "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100).mean())\n",
    "#as expected, a recommendation model that gives 0 scores for all items obtains a MRR score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQTJOmS5dB3i"
   },
   "outputs": [],
   "source": [
    "#note that mrr_score() displays a progress bar if you set verbose=True\n",
    "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100, verbose=True).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCWXwVC5Mtyj"
   },
   "source": [
    "# Part-A. Combination of Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyvGgW_3ZjLV"
   },
   "source": [
    "## Task 1. Explicit & Implicit Matrix Factorisation Models\n",
    "\n",
    "Create and train three matrix factorisation systems:\n",
    "\n",
    "(NOTE: Different models will be trained using DIFFERENT datasets)\n",
    " - \"EMF\": explicit MF, trained on the **ratings** Interactions object (`rating_dataset`)\n",
    " - \"IMF\": implicit MF, trained on the **toread** Interactions object (`toread_dataset_train`)\n",
    " - \"BPRMF\": implicit MF with the BPR loss function (`loss='bpr'`), trained on the **toread** Interactions object (`toread_dataset_train`)\n",
    "\n",
    "Use a variable of the same name for these models, as we will use some of them later (e.g. `BPRMF`).\n",
    "\n",
    "Normally, the hyper-parameters (e.g. `embedding_dim`) will be tuned using the `validation` set based on different models, but here, to simplify the excercie, we use a fixed setting of those hyper-parameters, and keep a fixed random seed.\n",
    "  \n",
    "In all cases, you must use the standard initialisation arguments, i.e.\n",
    "`n_iter=10, embedding_dim=32, use_cuda=False, random_state=np.random.RandomState(SEED)`.\n",
    "\n",
    "Evaluate each of these models in terms of Mean Reciprocal Rank on the test set. MRR can be obtained using:\n",
    "```python\n",
    "mrr_score(X, test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n",
    "```\n",
    "where X is an instance of a Spotlight model. Do NOT change the `k` or `train` arguments. You MUST use these arguments for MRR for all of the rest of this Exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WExWc-Y9tiBI"
   },
   "source": [
    "### Implement the explicit MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySwuwnpDtgDY"
   },
   "outputs": [],
   "source": [
    "# Add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnGQjHS7uFiQ"
   },
   "source": [
    "Now you can answer quiz question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGi8plPUttEp"
   },
   "source": [
    "### Implement the implicit MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUKU7qzctegb"
   },
   "outputs": [],
   "source": [
    "# Add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cPKAoqWuKIh"
   },
   "source": [
    "Now you can answer quiz question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sETn-utit1TN"
   },
   "source": [
    "### Implement the BPRMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDADjtepRvpJ"
   },
   "outputs": [],
   "source": [
    "# Add your solution here\n",
    "# use BPRMF as the name of your model\n",
    "BRMF = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSD9ZqakuLxv"
   },
   "source": [
    "Now you can answer quiz question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZHCOmfEDOGo"
   },
   "source": [
    "## Task 2. Hybrid Model\n",
    "\n",
    "In this task, you are expected to create new hybrid recommendation models that\n",
    "combine the two models in Task 1, namely IMF and BPRMF.\n",
    "\n",
    "(a) Linearly combine the *scores* from IMF and BPRMF.  Here please use **CombSUM** as your data fusion function, and you need normalise both input scores into the range 0..1 using [sklearn's minmax_scale() function](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) before combining them.\n",
    "\n",
    "(b) Apply a pipelining recommender, where the top 100 items are obtained from IMF and re-ranked using the scores of BPRMF. Items not returned by IMF get a score of 0.\n",
    "\n",
    "To implement these hybrid models, you should create new classes that abide by the Spotlight model contract (namely, it has a `predict(self, uid)` function that returns a score for *all* items).\n",
    "\n",
    "Evaluate each model in terms of MRR. How many users are improved, how many are degraded compared to the BPRMF baseline?\n",
    "\n",
    "Finally, pass your instantiated model object to the `test_Hybrid_a()` (for (a)) or `test_Hybrid_b()` (for (b)) functions, as appropriate, and record the results in the quiz. For example, if your model for (b) is called `pipeline`, then you would run:\n",
    "```python\n",
    "test_Hybrid_b(pipeline)\n",
    "```\n",
    "\n",
    "You now have sufficient information to answer the Task 2 quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j6JzIOHkYw9"
   },
   "outputs": [],
   "source": [
    "def test_Hybrid_a(combsumObj):\n",
    "  for i, u in enumerate([5, 20]):\n",
    "    print(\"Hybrid a test case %d\" % i)\n",
    "    print(np.count_nonzero(combsumObj.predict(u) > 1))\n",
    "\n",
    "def test_Hybrid_b(pipeObj):\n",
    "  for i, iid in enumerate([3, 0]):\n",
    "    print(\"Hybrid b test case %d\" % i)\n",
    "    print(pipeObj.predict(0)[iid])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_o7a1ppFZ7R"
   },
   "outputs": [],
   "source": [
    "# Add your solutions here and evaluate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROCBxlfKKkGA"
   },
   "outputs": [],
   "source": [
    "#Now test your hybrid approaches for the quiz\n",
    "\n",
    "#test_Hybrid_a(linearModel)\n",
    "#test_Hybrid_b(pipeModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf0K6GECM0LQ"
   },
   "source": [
    "# Part-B. Analysing Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gszqk2kIZLX"
   },
   "source": [
    "## Utility methods\n",
    "\n",
    "Below, we provide a function, `get_top_K(model, uid : int, k : int)` which, when provided with a Spotlight model, will provide the top k predictions for the specified uid. The iids, their scores, and their embeddings are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIyc_p_dIm1M"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "\n",
    "def get_top_K(model, uid : int, k : int) -> Tuple[ Sequence[int], Sequence[float],  np.ndarray ] :\n",
    "  #returns iids, their (normalised) scores in descending order, and item emebddings for the top k predictions of the given uid.\n",
    "\n",
    "  from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "  from scipy.stats import rankdata\n",
    "  # get scores from model\n",
    "  scores = model.predict(uid)\n",
    "\n",
    "  # map scores into rank 0..1 over the entire item space\n",
    "  scores = minmax_scale(scores)\n",
    "\n",
    "  #compute their ranks\n",
    "  ranks = rankdata(-scores)\n",
    "\n",
    "  # get and filter iids, scores and embeddings\n",
    "  rtr_scores = scores[ranks <= k]\n",
    "  rtr_iids = np.argwhere(ranks <= k).flatten()\n",
    "  if hasattr(model, '_net'):\n",
    "    embs = model._net.item_embeddings.weight[rtr_iids].detach()\n",
    "  else:\n",
    "    # not a model that has any embeddings\n",
    "    embs = np.zeros([k,1])\n",
    "\n",
    "  # identify correct ordering using numpy.argsort()\n",
    "  ordering = (-1*rtr_scores).argsort()\n",
    "\n",
    "  #return iids, scores and their embeddings in descending order of score\n",
    "  return rtr_iids[ordering], rtr_scores[ordering], embs[ordering]\n",
    "\n",
    "if BPRMF is not None: # BPRMF is the model name defined in Task 1\n",
    "  iids, scores, embs = get_top_K(BPRMF, 0, 10)\n",
    "  print(\"Returned iids: %s\" % str(iids))\n",
    "  print(\"Returned scores: %s\" % str(scores))\n",
    "  print(\"Returned embeddings: %s\" % str(embs))\n",
    "else:\n",
    "  print(\"You need to define BPRMF in Task 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVYiQRYaEh61"
   },
   "source": [
    "## Task 3. Evaluation of Non-personalised Models\n",
    "Implement the following four (non-personalised) baselines for ranking books based on their statistics:\n",
    " - Average rating, obtained from ratings_df, `ratings` column\n",
    " - Number of ratings, obtained from books_df (column `ratings_count`)\n",
    " - Number of 5* ratings, obtained from books_df (column `ratings_5`)\n",
    " - Fraction of 5* ratings, calculated from the two sources of evidence above, i.e (columns  `ratings_5` and `ratings_count`).\n",
    "\n",
    "Evaluate these in terms of MRR using the provided test data. You may use the StaticModel class below.\n",
    "\n",
    "Hints:\n",
    " - As in Exercise 2, the order of items returned by predict() is _critical_. You may wish to refer to iid_map.\n",
    " - For all models, you need to ensure that your values are not cast to ints. If you are extracting values from a Pandas series, it is advised to use [.astype(np.float32)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xop8aPfyFucw"
   },
   "outputs": [],
   "source": [
    "class StaticModel:\n",
    "\n",
    "  def __init__(self, staticscores):\n",
    "    self.numitems = len(staticscores)\n",
    "    #print(self.numitems)\n",
    "    assert isinstance(staticscores, np.ndarray), \"Expected a numpy array\"\n",
    "    assert staticscores.dtype == np.float32 or staticscores.dtype == np.float64, \"Expected a numpy array of floats\"\n",
    "    self.staticscores = staticscores\n",
    "\n",
    "  def predict(self, uid):\n",
    "    #this model returns the same scores for each user\n",
    "    return self.staticscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R1q-Zm7FVM9"
   },
   "outputs": [],
   "source": [
    "# Add your solution here\n",
    "# And answer the quiz questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZqRSixpGvfn"
   },
   "source": [
    "## Task 4. Qualiatively Examining Recommendations\n",
    "\n",
    "From now on, we will consider the `BPRMF` model.\n",
    "\n",
    "In Recommender Systems, the ground truth (i.e. our list of books that the user has added to their \"to_read\" shelf) can be very incomplete. For instance, this can be because the user is not aware of the book yet.\n",
    "\n",
    "For this reason, it is important to \"eyeball\" the recommendations, to understand what the system is surfacing, and whether the recommendations make sense. In this way, we understand if the recommendations are reasonable, even if they are for books that the user has not actually read according to the test dataset.\n",
    "\n",
    "First, write a function, which given a uid (int), prints the *title and authors* of:\n",
    " - (a) the books that the user has previously shelved (c.f. `toread_dataset_train`)\n",
    " - (b) the books that the user will read in the future (c.f. `test_dataset`)\n",
    " - (c) the top 10 books that the user were recommended by `BPRMF` - you can make use of `get_top_K()`.\n",
    "\n",
    "You can use the previously defined `getAuthorTitle()` function in your solution.\n",
    "You will also want to compare books in (c) with those in (a) and (b).\n",
    "\n",
    "Then, we will examine two specific users, namely uid 1805 (u336) and uid 179 (user u1331), to analyse if their recommendations make sense. Refer to the Task 4 quiz questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kg1eFa5GYv5c"
   },
   "outputs": [],
   "source": [
    "# Add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES_zHeCkNBeC"
   },
   "source": [
    "# Part-C. Diversity of Recommendations\n",
    "\n",
    "This part of the exercise is concerned with diversification, as covered in Lecture 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvBep-ROHWSX"
   },
   "source": [
    "## Task 5. Measuring Intra-List Diversity\n",
    "\n",
    "\n",
    "For the BPR implicit factorisation model, implement the Intra-list diversity measure (see Lecture 11) of the top 5 scored items based on their item embeddings in the `BPRMF` model.\n",
    "\n",
    "Implement your ILD as a function with the specification:\n",
    "```python\n",
    "def measure_ild(top_books : Sequence[int], K : int=5) -> float\n",
    "```\n",
    "where:\n",
    " - `top_books` is a list or a Numpy array of iids that have been returned for a particular user. For instance, it can be obtained from `get_top_K()`.\n",
    " - `K` is the number of top-ranked items to consider from `top_books`.\n",
    " - Your implementation should use the item embeddings stored in the `BPRMF` model.\n",
    "\n",
    "Calculate the ILD (with k=5). Using your code for Task 4, identify the books previously shelved and recommended for the specific users requested in the quiz, and use these to analyse the recommendations.\n",
    "\n",
    "Hints:\n",
    " - As can be seen in `get_top_K()`, item embeddings can be obtained from `BPRMF._net.item_embeddings.weight[iid]`.\n",
    " - For obtaining the cosine similarity of PyTorch tensors, use `nn.functional.cosine_similarity(, , axis=0)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n2vBwcnYuM4"
   },
   "outputs": [],
   "source": [
    "# Add your solution here\n",
    "def measure_ild(top_books : Sequence[int], K : int=5) -> float:\n",
    "  ILD = 0.0\n",
    "  return ILD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qwrP1jUpARF"
   },
   "source": [
    "## Task 6. Implement MMR Diversification\n",
    "\n",
    "Develop an Maximal Marginal Relevance (MMR) diversification technique, to re-rank the top-ranked recommendations for a given user.\n",
    "\n",
    "Your function should adhere to the specification as follows:\n",
    "```python\n",
    "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
    "```\n",
    "\n",
    "where:\n",
    " - iids is a list of iids,\n",
    " - scores are their corresponding scores (in descending order),\n",
    " - embs is their embeddings,\n",
    " - alpha controls the diversification tradeoff.\n",
    "\n",
    "The function returns a re-ordering of iids. As in previous Exercises, type hints are provided for clarity; a Sequence can be a list or numpy array.\n",
    "\n",
    "Hints:\n",
    " - As above, for obtaining the cosine similarity of PyTorch tensors, use nn.functional.cosine_similarity(, , axis=0).\n",
    "\n",
    "To use your `mmr()` function, provide it with the outputs of `get_top_K()`. For example, to obtain an MMR reordering of the top 10 predictions of uid 0, we can run:\n",
    "```\n",
    "mmr( *get_top_K(bprmodel, 0, 10), 0.5)\n",
    "```\n",
    "\n",
    "Thereafter, we provide test cases for your MMR implementation, which you  should report in the quiz. We also ask for the ILD values before and after the application of MMR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VkEMfRvIhKV"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
    "\n",
    "  assert len(iids) == len(scores)\n",
    "  assert len(iids) == embs.shape[0]\n",
    "  assert len(embs.size()) == 2\n",
    "\n",
    "\n",
    "  rtr_iids=iids\n",
    "\n",
    "  #input your solution here returns a re-ordering of iids, such that the first ranked item is first in the list\n",
    "\n",
    "  return rtr_iids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34QWxFVTfrLM"
   },
   "outputs": [],
   "source": [
    "def run_MMR_testcases(mmrfn):\n",
    "  example_embeddings1 = torch.tensor([[1.0,1.0],[1.0,1.0],[0,1.0],[0.1, 1.0]])\n",
    "  example_embeddings2 = torch.tensor([[1.0,1.0],[1.0,1.0],[0.02,1.0],[0.01,1.0]])\n",
    "  print(\"Testcase 0 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[0] )\n",
    "  print(\"Testcase 1 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[1] )\n",
    "  print(\"Testcase 2 : %s\" % mmrfn([1,2,3,4], [4, 3, 2, 1],  example_embeddings1, 1)[1] )\n",
    "  print(\"Testcase 3 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.001)[1] )\n",
    "  print(\"Testcase 4 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.5)[1] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfm9mCWZmBPQ"
   },
   "source": [
    "Now we can analyse the impact of our MMR implementation. Let's consider again uid 179 (user u1331).\n",
    "\n",
    "Apply MMR on the top 10 results obtained from the BPRMF model using `get_top_K()`, with an alpha value of 0.5. The following code should help:\n",
    "```python\n",
    "mmr( *get_top_K(BPRMF, 179, 10), 0.5)\n",
    "```\n",
    "\n",
    "Finally, anayse the returned books. Calculate the ILD (with `k=5`), and examine the authors and titles (using `getAuthorTitle()`).\n",
    "\n",
    "Now answer the questions in Task 6 of the Moodle quiz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wM7m8pOmCnM"
   },
   "outputs": [],
   "source": [
    "#add your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49QoGVphnegD"
   },
   "source": [
    "# Task 7 Content-related questions\n",
    "\n",
    "This task is not a practical task - instead there are questions that tests your understanding of some related content of the course in the quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8N-fEdkMKLD"
   },
   "source": [
    "# End of Exercise\n",
    "\n",
    "As part of your submission, you should complete the Exercise 3 quiz on Moodle.\n",
    "You will need to upload your notebook, complete with the **results** of executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9Rxt92Xeo12"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
